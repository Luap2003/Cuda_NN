{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import time\n",
    "\n",
    "# Load and preprocess the data\n",
    "def load_data():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values\n",
    "\n",
    "    # One-hot encode the labels\n",
    "    y_train_one_hot = to_categorical(y_train, num_classes=10)\n",
    "    y_test_one_hot = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "    return x_train, y_train_one_hot, x_test, y_test_one_hot\n",
    "\n",
    "# Build the neural network model\n",
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='SGD',\n",
    "                  loss='categorical_crossentropy',  # Changed loss function\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train and evaluate the model\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test, epochs=100, log_file='training_log_tf.csv'):\n",
    "    # Initialize CSVLogger\n",
    "    csv_logger = CSVLogger(log_file, append=True)\n",
    "\n",
    "    start_time = time.time()\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=64,\n",
    "        verbose=2,  # You can set to 1 or 2 for different verbosity levels\n",
    "        callbacks=[csv_logger]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    loss, test_accuracy = model.evaluate(x_test, y_test, verbose=2)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test Loss: {loss:.4f}\")\n",
    "    print(f\"Execution time: {end_time - start_time:.4f} seconds\")\n",
    "    return test_accuracy\n",
    "\n",
    "# Main function to run the pipeline\n",
    "def main():\n",
    "    x_train, y_train, x_test, y_test = load_data()\n",
    "    model = build_model()\n",
    "    train_and_evaluate_model(model, x_train, y_train, x_test, y_test)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "# Load and preprocess the data\n",
    "def load_data():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values\n",
    "\n",
    "    # One-hot encode the labels\n",
    "    y_train_one_hot = to_categorical(y_train, num_classes=10)\n",
    "    y_test_one_hot = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "    return x_train, y_train_one_hot, x_test, y_test_one_hot\n",
    "\n",
    "# Build the neural network model\n",
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(10, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='SGD',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Custom Callback to log extended metrics\n",
    "class ExtendedCSVLogger(Callback):\n",
    "    def __init__(self, filename, append=False):\n",
    "        super(ExtendedCSVLogger, self).__init__()\n",
    "        self.filename = filename\n",
    "        self.append = append\n",
    "        self.start_time = None\n",
    "        self.batches_per_epoch = 0\n",
    "\n",
    "        # Initialize CSV file with headers if not appending or file doesn't exist\n",
    "        if not append or not os.path.exists(filename):\n",
    "            with open(self.filename, 'w', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                headers = ['Epoch', 'Progress%', 'Loss', 'Accuracy%', 'Epoch Time(s)', 'Batches/s']\n",
    "                writer.writerow(headers)\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.start_time = time.time()\n",
    "        self.batches_per_epoch = 0\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        self.batches_per_epoch += 1\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        epoch_time = time.time() - self.start_time\n",
    "        # Calculate batches per second\n",
    "        if epoch_time > 0:\n",
    "            batches_per_sec = self.batches_per_epoch / epoch_time\n",
    "        else:\n",
    "            batches_per_sec = 0\n",
    "\n",
    "        # Calculate progress percentage\n",
    "        total_epochs = self.params.get('epochs', 1)\n",
    "        progress = ((epoch + 1) / total_epochs) * 100\n",
    "\n",
    "        # Prepare data to log with proper formatting\n",
    "        epoch_num = epoch + 1\n",
    "        progress_percent = f\"{progress:.2f}\"\n",
    "        loss = f\"{logs.get('loss', 0):.4f}\"\n",
    "        accuracy = f\"{logs.get('accuracy', 0) * 100:.2f}\"\n",
    "        epoch_time_sec = f\"{epoch_time:.2f}\"\n",
    "        batches_per_second = f\"{batches_per_sec:.2f}\"\n",
    "\n",
    "        # Write to CSV\n",
    "        with open(self.filename, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([epoch_num, progress_percent, loss, accuracy, epoch_time_sec, batches_per_second])\n",
    "\n",
    "# Train and evaluate the model\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test, epochs=100, log_file=None, batch_size = 64):\n",
    "    if log_file is None:\n",
    "        # Generate a unique identifier for the log file\n",
    "        unique_id = uuid.uuid4()\n",
    "        log_file = f'../logs/training_log_tf_{unique_id}_bs_64_ep100.csv'\n",
    "\n",
    "    # Initialize only ExtendedCSVLogger\n",
    "    extended_csv_logger = ExtendedCSVLogger(log_file, append=False)  # Set append=False to start fresh\n",
    "\n",
    "    start_time = time.time()\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=2,  # You can set to 1 or 2 for different verbosity levels\n",
    "        callbacks=[extended_csv_logger]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    loss, test_accuracy = model.evaluate(x_test, y_test, verbose=2)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test Loss: {loss:.4f}\")\n",
    "    print(f\"Total Execution time: {end_time - start_time:.4f} seconds\")\n",
    "    return test_accuracy\n",
    "\n",
    "# Main function to run the pipeline\n",
    "def main(run_id):\n",
    "    # Ensure the logs directory exists\n",
    "    os.makedirs('../logs', exist_ok=True)\n",
    "    batch_size = 128\n",
    "    x_train, y_train, x_test, y_test = load_data()\n",
    "    model = build_model()\n",
    "    log_file = f'../logs_256_64/training_log_tf_{int(time.time())}_bs{str(batch_size)}_ep100.csv'\n",
    "    train_and_evaluate_model(model, x_train, y_train, x_test, y_test, log_file=log_file, batch_size=batch_size)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for i in range(100):\n",
    "        main(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# List physical devices available to TensorFlow\n",
    "print(\"Available devices:\")\n",
    "for device in tf.config.list_physical_devices():\n",
    "    print(device)\n",
    "\n",
    "# Specifically check for GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"\\nTensorFlow is using the GPU(s):\")\n",
    "    for gpu in gpus:\n",
    "        print(gpu)\n",
    "else:\n",
    "    print(\"\\nNo GPU detected, TensorFlow is using the CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudnn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
